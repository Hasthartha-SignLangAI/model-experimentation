{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"   # hide GPU from TF\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"GPUs visible:\", tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAEI3776YMw2",
        "outputId": "20e1311f-f6b2-4ba5-f9fb-a531bc2e163b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPUs visible: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile, os\n",
        "\n",
        "ZIP_NAME = \"Dataset_V2.zip\"  # must match the uploaded filename\n",
        "\n",
        "with zipfile.ZipFile(ZIP_NAME, 'r') as z:\n",
        "    z.extractall(\"/content\")\n",
        "\n",
        "print(\"Extracted folders in /content:\")\n",
        "print([p for p in os.listdir(\"/content\") if \"Dataset\" in p or \"dataset\" in p])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1PUuqvhnAtk",
        "outputId": "f3ddd495-4fb1-433a-ab16-5c7fc111b081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted folders in /content:\n",
            "['Dataset_V2.zip', 'Dataset_V2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkzG2SVCl0nm",
        "outputId": "fd4628fb-0dab-48ec-b303-ebec6fff8b6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUT_DIR: /content/out_metric\n"
          ]
        }
      ],
      "source": [
        "import os, glob, json, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# ====== EDIT THESE PATHS ======\n",
        "DATASET_ROOT = \"Dataset_V2\"   # <-- root containing words (including idle)\n",
        "OUT_DIR = \"/content/out_metric\"\n",
        "\n",
        "# Day-wise split\n",
        "TRAIN_DAYS = {f\"Day_{i}\" for i in range(1, 9)}   # Day_1..Day_8\n",
        "VAL_DAYS   = {\"Day_9\"}\n",
        "TEST_DAYS  = {\"Day_10\"}\n",
        "\n",
        "# Window length used in your project\n",
        "T = 512\n",
        "F = 9  # features after dropping timestamp\n",
        "\n",
        "# Training\n",
        "EMBED_DIM = 128\n",
        "BATCH_TRIPLETS = 64         # number of triplets per batch\n",
        "STEPS_PER_EPOCH = 200       # triplet batches per epoch\n",
        "EPOCHS = 25\n",
        "LR = 1e-3\n",
        "MARGIN = 0.4                # triplet margin (0.2..0.6 works)\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv_robust(fp, expected_cols=10):\n",
        "    with open(fp, \"rb\") as f:\n",
        "        raw = f.read().replace(b\"\\x00\", b\"\")\n",
        "    text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "    good_rows = []\n",
        "    for line in text.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        while line.endswith(\",\"):\n",
        "            line = line[:-1].strip()\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        if len(parts) != expected_cols:\n",
        "            continue\n",
        "        if any(p == \"\" for p in parts):\n",
        "            continue\n",
        "        try:\n",
        "            good_rows.append([float(p) for p in parts])\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    if not good_rows:\n",
        "        raise ValueError(f\"No valid numeric rows in {fp}\")\n",
        "    return np.array(good_rows, dtype=np.float32)\n",
        "\n",
        "def moving_average(x, w=25):\n",
        "    w = max(1, int(w))\n",
        "    kernel = np.ones(w, dtype=np.float32) / w\n",
        "    return np.convolve(x, kernel, mode=\"same\")\n",
        "\n",
        "def fix_length_center(X, target_len):\n",
        "    if len(X) >= target_len:\n",
        "        start = (len(X) - target_len) // 2\n",
        "        return X[start:start + target_len]\n",
        "    pad = np.zeros((target_len - len(X), X.shape[1]), dtype=X.dtype)\n",
        "    return np.vstack([X, pad])\n",
        "\n",
        "def emg_dc_remove(X):\n",
        "    X = X.copy()\n",
        "    X[:, :3] -= X[:, :3].mean(axis=0, keepdims=True)\n",
        "    return X\n",
        "\n",
        "def crop_active_region_emg(X, target_len=512, smooth_w=25, thresh_ratio=0.25):\n",
        "    Traw = X.shape[0]\n",
        "    if Traw == 0:\n",
        "        return np.zeros((target_len, X.shape[1]), dtype=np.float32)\n",
        "\n",
        "    energy = np.sum(np.abs(X[:, :3]), axis=1)\n",
        "    energy_s = moving_average(energy, w=smooth_w)\n",
        "\n",
        "    mx = float(np.max(energy_s))\n",
        "    if mx <= 1e-6:\n",
        "        return fix_length_center(X, target_len)\n",
        "\n",
        "    thresh = thresh_ratio * mx\n",
        "    active = np.where(energy_s >= thresh)[0]\n",
        "    if len(active) < 5:\n",
        "        return fix_length_center(X, target_len)\n",
        "\n",
        "    start = int(active[0])\n",
        "    end   = int(active[-1])\n",
        "    center = (start + end) // 2\n",
        "\n",
        "    half = target_len // 2\n",
        "    win_start = max(0, center - half)\n",
        "    win_end = win_start + target_len\n",
        "    if win_end > Traw:\n",
        "        win_end = Traw\n",
        "        win_start = max(0, win_end - target_len)\n",
        "\n",
        "    cropped = X[win_start:win_end]\n",
        "    if cropped.shape[0] < target_len:\n",
        "        pad = np.zeros((target_len - cropped.shape[0], X.shape[1]), dtype=cropped.dtype)\n",
        "        cropped = np.vstack([cropped, pad])\n",
        "    return cropped\n",
        "\n",
        "def load_one_sample(path):\n",
        "    arr = load_csv_robust(path, expected_cols=10)  # (Traw, 10)\n",
        "    X = arr[:, 1:]                                # drop timestamp -> (Traw, 9)\n",
        "    X = emg_dc_remove(X)\n",
        "    X = crop_active_region_emg(X, target_len=T, smooth_w=25, thresh_ratio=0.25)\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "4c3JZdo8mPAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_word_dirs(root):\n",
        "    return sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))])\n",
        "\n",
        "def build_label_map(root):\n",
        "    labels = list_word_dirs(root)\n",
        "    return {lbl: i for i, lbl in enumerate(labels)}\n",
        "\n",
        "def collect_files_with_meta(root):\n",
        "    label2id = build_label_map(root)\n",
        "    items = []  # (filepath, label_id, day_name)\n",
        "\n",
        "    for label, lab_id in label2id.items():\n",
        "        class_dir = os.path.join(root, label)\n",
        "        files = sorted(glob.glob(os.path.join(class_dir, \"**\", \"*.txt\"), recursive=True))\n",
        "        for fp in files:\n",
        "            day_name = os.path.basename(os.path.dirname(fp))  # Day_1 folder\n",
        "            items.append((fp, lab_id, day_name, label))\n",
        "    return items, label2id\n",
        "\n",
        "items, label2id = collect_files_with_meta(DATASET_ROOT)\n",
        "id2label = {v:k for k,v in label2id.items()}\n",
        "\n",
        "print(\"Classes:\", len(label2id))\n",
        "print(\"Example labels:\", list(label2id.keys())[:10])\n",
        "print(\"Total files:\", len(items))\n",
        "\n",
        "days_present = sorted(set([d for _,_,d,_ in items]))\n",
        "print(\"Days found:\", days_present)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWVxH2cAmP0I",
        "outputId": "b45ec51e-a52e-4b76-c0c8-5904b5002629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: 11\n",
            "Example labels: ['ada', 'awidinawa', 'boru', 'hawasa', 'hodai', 'idle', 'irida', 'narakai', 'pata', 'saduda']\n",
            "Total files: 1100\n",
            "Days found: ['Day_1', 'Day_10', 'Day_2', 'Day_3', 'Day_4', 'Day_5', 'Day_6', 'Day_7', 'Day_8', 'Day_9']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_items = [it for it in items if it[2] in TRAIN_DAYS]\n",
        "val_items   = [it for it in items if it[2] in VAL_DAYS]\n",
        "test_items  = [it for it in items if it[2] in TEST_DAYS]\n",
        "\n",
        "print(\"Train:\", len(train_items), \"Val:\", len(val_items), \"Test:\", len(test_items))\n",
        "\n",
        "# Fit scaler using train data only (streaming)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# We’ll sample some train files to fit quickly (or fit all if small)\n",
        "FIT_MAX = min(800, len(train_items))  # you can increase to fit all\n",
        "fit_subset = random.sample(train_items, FIT_MAX) if len(train_items) > FIT_MAX else train_items\n",
        "\n",
        "all_rows = []\n",
        "for fp, y, day, label in fit_subset:\n",
        "    try:\n",
        "        X = load_one_sample(fp)  # (512,9)\n",
        "        all_rows.append(X)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "X_fit = np.concatenate(all_rows, axis=0)  # (N*512,9)\n",
        "scaler.fit(X_fit)\n",
        "\n",
        "scaler_params = {\"mean\": scaler.mean_.tolist(), \"scale\": scaler.scale_.tolist()}\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "with open(os.path.join(OUT_DIR, \"scaler_params.json\"), \"w\") as f:\n",
        "    json.dump(scaler_params, f, indent=2)\n",
        "\n",
        "with open(os.path.join(OUT_DIR, \"label_map.json\"), \"w\") as f:\n",
        "    json.dump(label2id, f, indent=2)\n",
        "\n",
        "print(\"Saved scaler_params.json and label_map.json to\", OUT_DIR)\n",
        "\n",
        "def normalize(X):\n",
        "    return (X - scaler.mean_) / (scaler.scale_ + 1e-6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj25dDIfmXwU",
        "outputId": "c7a776d2-a54a-4978-da91-98de6eedd94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 881 Val: 110 Test: 109\n",
            "Saved scaler_params.json and label_map.json to /content/out_metric\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_encoder(T=512, F=9, embed_dim=128):\n",
        "    inp = layers.Input(shape=(T, F))\n",
        "\n",
        "    x = layers.Conv1D(64, 5, padding=\"same\")(inp)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPool1D(2)(x)\n",
        "\n",
        "    x = layers.Conv1D(128, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPool1D(2)(x)\n",
        "\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.LSTM(128, implementation=2)(x)  # ✅ CPU-safe\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # Embedding\n",
        "    emb = layers.Dense(embed_dim)(x)\n",
        "\n",
        "    # ✅ FIX: wrap TF op in Lambda\n",
        "    emb = layers.Lambda(\n",
        "        lambda t: tf.nn.l2_normalize(t, axis=-1),\n",
        "        name=\"l2_norm\"\n",
        "    )(emb)\n",
        "\n",
        "    return models.Model(inp, emb, name=\"cnn_lstm_encoder\")\n"
      ],
      "metadata": {
        "id": "vBlOx4eFmZ3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Create label -> list of filepaths for TRAIN\n",
        "train_by_label = defaultdict(list)\n",
        "for fp, y, day, label in train_items:\n",
        "    train_by_label[y].append(fp)\n",
        "\n",
        "# Filter labels with >=2 samples (needed for anchor+positive)\n",
        "valid_labels = [y for y, fps in train_by_label.items() if len(fps) >= 2]\n",
        "print(\"Valid labels for triplets:\", len(valid_labels), \"/\", len(label2id))\n",
        "\n",
        "def sample_triplet():\n",
        "    # Choose anchor label\n",
        "    a_lab = random.choice(valid_labels)\n",
        "    pos_list = train_by_label[a_lab]\n",
        "    a_fp, p_fp = random.sample(pos_list, 2)\n",
        "\n",
        "    # Choose negative label != a_lab\n",
        "    n_lab = random.choice(valid_labels)\n",
        "    while n_lab == a_lab:\n",
        "        n_lab = random.choice(valid_labels)\n",
        "    n_fp = random.choice(train_by_label[n_lab])\n",
        "\n",
        "    # Load + preprocess + normalize\n",
        "    A = normalize(load_one_sample(a_fp)).astype(np.float32)\n",
        "    P = normalize(load_one_sample(p_fp)).astype(np.float32)\n",
        "    N = normalize(load_one_sample(n_fp)).astype(np.float32)\n",
        "    return A, P, N\n",
        "\n",
        "def triplet_batch(batch_size=BATCH_TRIPLETS):\n",
        "    A_list, P_list, N_list = [], [], []\n",
        "    for _ in range(batch_size):\n",
        "        try:\n",
        "            A, P, N = sample_triplet()\n",
        "            A_list.append(A); P_list.append(P); N_list.append(N)\n",
        "        except:\n",
        "            # if a file fails, just resample\n",
        "            continue\n",
        "    A = np.stack(A_list, axis=0)\n",
        "    P = np.stack(P_list, axis=0)\n",
        "    N = np.stack(N_list, axis=0)\n",
        "    return A, P, N\n",
        "\n",
        "# Quick sanity\n",
        "A,P,N = triplet_batch(8)\n",
        "print(A.shape, P.shape, N.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcQx9uIZmanv",
        "outputId": "9c8cf137-0dff-4d29-d085-b7279b392a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid labels for triplets: 11 / 11\n",
            "(8, 512, 9) (8, 512, 9) (8, 512, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build encoder first\n",
        "encoder = build_encoder(T=T, F=F, embed_dim=EMBED_DIM)\n",
        "\n",
        "# Quick sanity check\n",
        "print(\"Encoder built:\", type(encoder))\n",
        "encoder.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "Q69hHM_mryI4",
        "outputId": "4a8683a7-bba4-42ab-c1d1-1ee9e77f2c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder built: <class 'keras.src.models.functional.Functional'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"cnn_lstm_encoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cnn_lstm_encoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m9\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m2,944\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu (\u001b[38;5;33mReLU\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m24,704\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ l2_norm (\u001b[38;5;33mLambda\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ l2_norm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m193,024\u001b[0m (754.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">193,024</span> (754.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m192,640\u001b[0m (752.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192,640</span> (752.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(LR)\n",
        "\n",
        "@tf.function\n",
        "def train_step(encoder, A, P, N):\n",
        "    with tf.GradientTape() as tape:\n",
        "        eA = encoder(A, training=True)\n",
        "        eP = encoder(P, training=True)\n",
        "        eN = encoder(N, training=True)\n",
        "\n",
        "        d_ap = tf.reduce_sum(tf.square(eA - eP), axis=1)\n",
        "        d_an = tf.reduce_sum(tf.square(eA - eN), axis=1)\n",
        "        loss = tf.reduce_mean(tf.nn.relu(d_ap - d_an + MARGIN))\n",
        "\n",
        "    grads = tape.gradient(loss, encoder.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, encoder.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    losses = []\n",
        "    for step in range(STEPS_PER_EPOCH):\n",
        "        A, P, N = triplet_batch(BATCH_TRIPLETS)\n",
        "        loss = train_step(encoder,\n",
        "                          tf.constant(A, dtype=tf.float32),\n",
        "                          tf.constant(P, dtype=tf.float32),\n",
        "                          tf.constant(N, dtype=tf.float32))\n",
        "        losses.append(float(loss.numpy()))\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}  loss={np.mean(losses):.4f}\")\n"
      ],
      "metadata": {
        "id": "aR2hjczbmdoo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f335eefb-7afe-4b17-a52c-112cdf533782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25  loss=0.2390\n",
            "Epoch 2/25  loss=0.1450\n",
            "Epoch 3/25  loss=0.1113\n",
            "Epoch 4/25  loss=0.0731\n",
            "Epoch 5/25  loss=0.0422\n",
            "Epoch 6/25  loss=0.0382\n",
            "Epoch 7/25  loss=0.0209\n",
            "Epoch 8/25  loss=0.0361\n",
            "Epoch 9/25  loss=0.0230\n",
            "Epoch 10/25  loss=0.0189\n",
            "Epoch 11/25  loss=0.0165\n",
            "Epoch 12/25  loss=0.0120\n",
            "Epoch 13/25  loss=0.0078\n",
            "Epoch 14/25  loss=0.0137\n",
            "Epoch 15/25  loss=0.0064\n",
            "Epoch 16/25  loss=0.0013\n",
            "Epoch 17/25  loss=0.0019\n",
            "Epoch 18/25  loss=0.0125\n",
            "Epoch 19/25  loss=0.0109\n",
            "Epoch 20/25  loss=0.0093\n",
            "Epoch 21/25  loss=0.0050\n",
            "Epoch 22/25  loss=0.0012\n",
            "Epoch 23/25  loss=0.0014\n",
            "Epoch 24/25  loss=0.0019\n",
            "Epoch 25/25  loss=0.0018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.save_weights(\"/content/encoder_triplet.weights.h5\")\n",
        "print(\"Saved:\", \"/content/encoder_triplet.weights.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odUb2nD5_NN9",
        "outputId": "a23459b3-0560-4ee3-b93c-51b75ee486dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/encoder_triplet.weights.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.save(\"/content/encoder_triplet.keras\")\n"
      ],
      "metadata": {
        "id": "pUgptvW-_-T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.export(\"/content/encoder_savedmodel_cpu\")\n",
        "!zip -r encoder_savedmodel_cpu.zip /content/encoder_savedmodel_cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWCoAtBs_Zbq",
        "outputId": "c3bbe4bd-ac26-4daa-e3a6-256cda557ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/content/encoder_savedmodel_cpu'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 512, 9), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 128), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  135036371382544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036371382160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036371384464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036371382352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036371381392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036371385424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036371385616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036371386192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036371384080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036371384656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036371384848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036371386000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036367783696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036367784080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036367785232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036367783888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036367784272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036367785040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135036367786384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  adding: content/encoder_savedmodel_cpu/ (stored 0%)\n",
            "  adding: content/encoder_savedmodel_cpu/variables/ (stored 0%)\n",
            "  adding: content/encoder_savedmodel_cpu/variables/variables.index (deflated 69%)\n",
            "  adding: content/encoder_savedmodel_cpu/variables/variables.data-00000-of-00001 (deflated 8%)\n",
            "  adding: content/encoder_savedmodel_cpu/fingerprint.pb (stored 0%)\n",
            "  adding: content/encoder_savedmodel_cpu/saved_model.pb (deflated 86%)\n",
            "  adding: content/encoder_savedmodel_cpu/assets/ (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ENC_DIR = os.path.join(OUT_DIR, \"encoder_savedmodel\")\n",
        "encoder.export(ENC_DIR)\n",
        "print(\"Saved encoder to:\", ENC_DIR)\n",
        "\n",
        "!zip -r /content/encoder_savedmodel.zip {ENC_DIR}\n",
        "print(\"Zipped -> /content/encoder_savedmodel.zip\")\n"
      ],
      "metadata": {
        "id": "ci1cTDvkmgoo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ff91c2-f2f4-4ec8-8619-4f05444dbddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/content/out_metric/encoder_savedmodel'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 512, 9), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 128), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136598910087440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598910086864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598910089168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598910087248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598910087056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598910090128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598910090320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598910090512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598910088400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598910090704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598910088976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598910090896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598903014992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598903015376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598903016528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598903015184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598903015568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598903016336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136598903017680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Saved encoder to: /content/out_metric/encoder_savedmodel\n",
            "  adding: content/out_metric/encoder_savedmodel/ (stored 0%)\n",
            "  adding: content/out_metric/encoder_savedmodel/saved_model.pb (deflated 86%)\n",
            "  adding: content/out_metric/encoder_savedmodel/fingerprint.pb (stored 0%)\n",
            "  adding: content/out_metric/encoder_savedmodel/assets/ (stored 0%)\n",
            "  adding: content/out_metric/encoder_savedmodel/variables/ (stored 0%)\n",
            "  adding: content/out_metric/encoder_savedmodel/variables/variables.data-00000-of-00001 (deflated 8%)\n",
            "  adding: content/out_metric/encoder_savedmodel/variables/variables.index (deflated 69%)\n",
            "Zipped -> /content/encoder_savedmodel.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile, os\n",
        "\n",
        "ZIP_PATH = \"/content/FewShot.zip\"\n",
        "OUT_DIR  = \"/content\"\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "    z.extractall(OUT_DIR)\n",
        "\n",
        "print(\"✅ Extracted. Top-level folders now:\")\n",
        "print([p for p in os.listdir(\"/content\") if os.path.isdir(\"/content/\"+p)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv05VJljXsvc",
        "outputId": "e9ce2cff-296f-4bc3-b81c-96f51eaa0ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extracted. Top-level folders now:\n",
            "['.config', 'encoder_savedmodel_cpu', 'FewShot', '__MACOSX', 'Dataset_V2', 'out_metric', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# =====================\n",
        "# PATHS (from your screenshot)\n",
        "# =====================\n",
        "ENCODER_DIR  = \"/content/encoder_savedmodel_cpu\"\n",
        "SCALER_PATH  = \"/content/out_metric/scaler_params.json\"\n",
        "FEWSHOT_ROOT = \"/content/FewShot\"\n",
        "OUT_DB       = \"/content/fewshot_db.json\"\n",
        "\n",
        "T = 512\n",
        "F = 9\n",
        "\n",
        "# =====================\n",
        "# Load scaler params\n",
        "# =====================\n",
        "with open(SCALER_PATH) as f:\n",
        "    sc = json.load(f)\n",
        "mean  = np.array(sc[\"mean\"], dtype=np.float32)\n",
        "scale = np.array(sc[\"scale\"], dtype=np.float32)\n",
        "\n",
        "def normalize(X):\n",
        "    return (X - mean) / (scale + 1e-6)\n",
        "\n",
        "# =====================\n",
        "# Same robust loader + crop used in training\n",
        "# =====================\n",
        "def load_csv_robust(fp, expected_cols=10):\n",
        "    with open(fp, \"rb\") as f:\n",
        "        raw = f.read().replace(b\"\\x00\", b\"\")\n",
        "    text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "    good_rows = []\n",
        "    for line in text.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        while line.endswith(\",\"):\n",
        "            line = line[:-1].strip()\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        if len(parts) != expected_cols:\n",
        "            continue\n",
        "        if any(p == \"\" for p in parts):\n",
        "            continue\n",
        "        try:\n",
        "            good_rows.append([float(p) for p in parts])\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    if not good_rows:\n",
        "        raise ValueError(f\"No valid numeric rows in {fp}\")\n",
        "    return np.array(good_rows, dtype=np.float32)\n",
        "\n",
        "def moving_average(x, w=25):\n",
        "    w = max(1, int(w))\n",
        "    kernel = np.ones(w, dtype=np.float32) / w\n",
        "    return np.convolve(x, kernel, mode=\"same\")\n",
        "\n",
        "def fix_length_center(X, target_len):\n",
        "    if len(X) >= target_len:\n",
        "        start = (len(X) - target_len) // 2\n",
        "        return X[start:start + target_len]\n",
        "    pad = np.zeros((target_len - len(X), X.shape[1]), dtype=X.dtype)\n",
        "    return np.vstack([X, pad])\n",
        "\n",
        "def emg_dc_remove(X):\n",
        "    X = X.copy()\n",
        "    X[:, :3] -= X[:, :3].mean(axis=0, keepdims=True)\n",
        "    return X\n",
        "\n",
        "def crop_active_region_emg(X, target_len=512, smooth_w=25, thresh_ratio=0.25):\n",
        "    Traw = X.shape[0]\n",
        "    if Traw == 0:\n",
        "        return np.zeros((target_len, X.shape[1]), dtype=np.float32)\n",
        "\n",
        "    energy = np.sum(np.abs(X[:, :3]), axis=1)\n",
        "    energy_s = moving_average(energy, w=smooth_w)\n",
        "\n",
        "    mx = float(np.max(energy_s))\n",
        "    if mx <= 1e-6:\n",
        "        return fix_length_center(X, target_len)\n",
        "\n",
        "    thresh = thresh_ratio * mx\n",
        "    active = np.where(energy_s >= thresh)[0]\n",
        "    if len(active) < 5:\n",
        "        return fix_length_center(X, target_len)\n",
        "\n",
        "    start = int(active[0])\n",
        "    end   = int(active[-1])\n",
        "    center = (start + end) // 2\n",
        "\n",
        "    half = target_len // 2\n",
        "    win_start = max(0, center - half)\n",
        "    win_end = win_start + target_len\n",
        "    if win_end > Traw:\n",
        "        win_end = Traw\n",
        "        win_start = max(0, win_end - target_len)\n",
        "\n",
        "    cropped = X[win_start:win_end]\n",
        "    if cropped.shape[0] < target_len:\n",
        "        pad = np.zeros((target_len - cropped.shape[0], X.shape[1]), dtype=cropped.dtype)\n",
        "        cropped = np.vstack([cropped, pad])\n",
        "    return cropped\n",
        "\n",
        "def load_one_sample(fp):\n",
        "    arr = load_csv_robust(fp, expected_cols=10)  # (Traw,10)\n",
        "    X = arr[:, 1:]                               # drop timestamp -> (Traw,9)\n",
        "    X = emg_dc_remove(X)\n",
        "    X = crop_active_region_emg(X, target_len=T)\n",
        "    X = normalize(X)\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "# =====================\n",
        "# Load encoder SavedModel signature\n",
        "# =====================\n",
        "loaded = tf.saved_model.load(ENCODER_DIR)\n",
        "infer = loaded.signatures[\"serving_default\"]\n",
        "\n",
        "IN_KEY  = list(infer.structured_input_signature[1].keys())[0]\n",
        "OUT_KEY = list(infer.structured_outputs.keys())[0]\n",
        "\n",
        "print(\"Encoder signature:\")\n",
        "print(\"  IN_KEY :\", IN_KEY)\n",
        "print(\"  OUT_KEY:\", OUT_KEY)\n",
        "\n",
        "def embed(X):  # X: (T,9)\n",
        "    Xb = X[np.newaxis, ...].astype(np.float32)\n",
        "    out = infer(**{IN_KEY: tf.constant(Xb)})\n",
        "    emb = out[OUT_KEY].numpy()[0]\n",
        "    emb = emb / (np.linalg.norm(emb) + 1e-9)\n",
        "    return emb.astype(np.float32)\n",
        "\n",
        "# =====================\n",
        "# Build prototypes\n",
        "# =====================\n",
        "proto = {}\n",
        "stats = {}\n",
        "\n",
        "words = sorted([d for d in os.listdir(FEWSHOT_ROOT) if os.path.isdir(os.path.join(FEWSHOT_ROOT, d))])\n",
        "print(\"Found few-shot word folders:\", words)\n",
        "\n",
        "for w in words:\n",
        "    files = sorted(glob.glob(os.path.join(FEWSHOT_ROOT, w, \"*.txt\")))\n",
        "    if not files:\n",
        "        print(\"⚠️ No .txt files in\", w)\n",
        "        continue\n",
        "\n",
        "    embs = []\n",
        "    for fp in files:\n",
        "        try:\n",
        "            X = load_one_sample(fp)\n",
        "            embs.append(embed(X))\n",
        "        except Exception as e:\n",
        "            print(\"[SKIP]\", fp, \"->\", e)\n",
        "\n",
        "    if not embs:\n",
        "        print(\"⚠️ No valid embeddings for\", w)\n",
        "        continue\n",
        "\n",
        "    E = np.stack(embs, axis=0)\n",
        "    p = E.mean(axis=0)\n",
        "    p = p / (np.linalg.norm(p) + 1e-9)\n",
        "\n",
        "    proto[w] = p.tolist()\n",
        "    stats[w] = {\"n\": int(len(embs))}\n",
        "\n",
        "print(\"✅ Prototypes built:\", stats)\n",
        "\n",
        "with open(OUT_DB, \"w\") as f:\n",
        "    json.dump({\"prototypes\": proto, \"stats\": stats}, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", OUT_DB)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kbVOajJX4JT",
        "outputId": "57ee5994-4172-46a9-fa33-51ab69f3def3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder signature:\n",
            "  IN_KEY : keras_tensor\n",
            "  OUT_KEY: output_0\n",
            "Found few-shot word folders: ['amma', 'nil']\n",
            "✅ Prototypes built: {'amma': {'n': 10}, 'nil': {'n': 10}}\n",
            "Saved: /content/fewshot_db.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, numpy as np\n",
        "db = json.load(open(\"/content/fewshot_db.json\"))\n",
        "P = {k: np.array(v, dtype=np.float32) for k,v in db[\"prototypes\"].items()}\n",
        "keys = list(P.keys())\n",
        "for i in range(len(keys)):\n",
        "    for j in range(i+1, len(keys)):\n",
        "        a,b = keys[i], keys[j]\n",
        "        cos = float(np.dot(P[a], P[b]) / (np.linalg.norm(P[a])*np.linalg.norm(P[b]) + 1e-9))\n",
        "        print(a, \"vs\", b, \"cosine=\", round(cos, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i21rsliYRc6",
        "outputId": "aac478a1-f77c-4f62-9b9b-ab3c0860eb8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amma vs nil cosine= 0.4669\n"
          ]
        }
      ]
    }
  ]
}